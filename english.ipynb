{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/english-newari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN</th>\n",
       "      <th>en</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>welcome to Ideax</td>\n",
       "      <td>Ideax ए लसकुस​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a test</td>\n",
       "      <td>थो test ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>wa are tongue techies</td>\n",
       "      <td>जिपि tongue techies ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>this is just a demo</td>\n",
       "      <td>थो demo जक ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I am from Urlabari</td>\n",
       "      <td>जि उर्लाबारी च्वंम्ह</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SN                     en                     new\n",
       "0 NaN       welcome to Ideax          Ideax ए लसकुस​\n",
       "1 NaN         this is a test              थो test ख​\n",
       "2 NaN  wa are tongue techies  जिपि tongue techies ख​\n",
       "3 NaN    this is just a demo           थो demo जक ख​\n",
       "4 NaN     I am from Urlabari    जि उर्लाबारी च्वंम्ह"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['en','new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome to Ideax</td>\n",
       "      <td>Ideax ए लसकुस​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a test</td>\n",
       "      <td>थो test ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wa are tongue techies</td>\n",
       "      <td>जिपि tongue techies ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is just a demo</td>\n",
       "      <td>थो demo जक ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am from Urlabari</td>\n",
       "      <td>जि उर्लाबारी च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>lets go to see Indra Jatra</td>\n",
       "      <td>नु ईन्द्रजात्रा स्वो वोने ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>Lakhe</td>\n",
       "      <td>लशिँ – पुलुकिशि</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>You are like a lakhe</td>\n",
       "      <td>छ लाखे थें चोँ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Is your beloved well?</td>\n",
       "      <td>छिमी यज्जु म्ह फु ला ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>I am fine</td>\n",
       "      <td>जी म्ह फु ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              en                          new\n",
       "0               welcome to Ideax               Ideax ए लसकुस​\n",
       "1                 this is a test                   थो test ख​\n",
       "2          wa are tongue techies       जिपि tongue techies ख​\n",
       "3            this is just a demo                थो demo जक ख​\n",
       "4             I am from Urlabari         जि उर्लाबारी च्वंम्ह\n",
       "...                          ...                          ...\n",
       "1006  lets go to see Indra Jatra  नु ईन्द्रजात्रा स्वो वोने ।\n",
       "1007                       Lakhe              लशिँ – पुलुकिशि\n",
       "1008        You are like a lakhe             छ लाखे थें चोँ ।\n",
       "1009       Is your beloved well?       छिमी यज्जु म्ह फु ला ?\n",
       "1010                   I am fine                  जी म्ह फु ।\n",
       "\n",
       "[1011 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65914/2664996961.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"en\"] = df[\"en\"].apply(process_text)\n",
      "/tmp/ipykernel_65914/2664996961.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"new\"] = df[\"new\"].apply(clean_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome to ideax</td>\n",
       "      <td>Ideax ए लसकुस​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a test</td>\n",
       "      <td>थो test ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wa are tongue techies</td>\n",
       "      <td>जिपि tongue techies ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is just a demo</td>\n",
       "      <td>थो demo जक ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am from urlabari</td>\n",
       "      <td>जि उर्लाबारी च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i am from damak</td>\n",
       "      <td>जि दमक च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i am from birtamod</td>\n",
       "      <td>जि बिर्तामोड च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my name is nishant</td>\n",
       "      <td>जिगू नां निशान्त खः।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>my name is drishya</td>\n",
       "      <td>जिगू नां दृश्य खः।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hello my name is sushan</td>\n",
       "      <td>ज्वजलपा जिगू नां सुशन खः।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        en                        new\n",
       "0         welcome to ideax             Ideax ए लसकुस​\n",
       "1           this is a test                 थो test ख​\n",
       "2    wa are tongue techies     जिपि tongue techies ख​\n",
       "3      this is just a demo              थो demo जक ख​\n",
       "4       i am from urlabari       जि उर्लाबारी च्वंम्ह\n",
       "5          i am from damak             जि दमक च्वंम्ह\n",
       "6       i am from birtamod       जि बिर्तामोड च्वंम्ह\n",
       "7       my name is nishant       जिगू नां निशान्त खः।\n",
       "8       my name is drishya         जिगू नां दृश्य खः।\n",
       "9  hello my name is sushan  ज्वजलपा जिगू नां सुशन खः।"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessing(df):\n",
    "    def process_text(text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    df[\"en\"] = df[\"en\"].apply(process_text)\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'[०-९]', '', text)\n",
    "        text = re.sub(r'[()#/@;:<>‘+=।?!|,’‘’]\".', '', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    df[\"new\"] = df[\"new\"].apply(clean_text)    \n",
    "    return df\n",
    "data = preprocessing(data)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     0\n",
       "new    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2AA8C327A8C2F07D/Programming/Python/translationModel/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_nepali = AutoTokenizer.from_pretrained('sakonii/deberta-base-nepali')\n",
    "\n",
    "def newari_tokenizer(sentence):\n",
    "    tokens = tokenizer_nepali.tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "# Create source and target language tokenizer.\n",
    "SRC_LANGUAGE = 'new'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Placeholders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer(newari_tokenizer)\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:    \n",
    "    for index,data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = df.iterrows()\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import math\n",
    "from torch.nn import Transformer\n",
    "import torch.nn as nn \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Check whether running on gpu or cpu\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size) \n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "   \n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2AA8C327A8C2F07D/Programming/Python/translationModel/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8 \n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 10\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "DROP_OUT = 0.1\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM,DROP_OUT)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "     \n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    \n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to tran test set\n",
    "split_ratio = 0.9\n",
    "split = round(df.shape[0]* split_ratio)\n",
    "train = df.iloc[:split]\n",
    "train_ds = list(zip(train['new'],train['en']))\n",
    "valid = df.iloc[split:]\n",
    "val_ds = list(zip(valid['new'],valid['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "accumulation_steps = 5\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    val_los = 0\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    optimizer.zero_grad() \n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)   \n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss = loss / accumulation_steps # Normalize our loss (if averaged)\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            optimizer.step() # Now we can do an optimizer step\n",
    "            optimizer.zero_grad() # Reset gradients tensor  \n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    #val_iter = valid.iterrows()\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss = loss / accumulation_steps # Normalize our loss (if averaged)\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/2AA8C327A8C2F07D/Programming/Python/translationModel/venv/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.209, Val loss: 1.186, Epoch time = 3.746s\n",
      "Epoch: 2, Train loss: 1.116, Val loss: 1.190, Epoch time = 2.793s\n",
      "Epoch: 3, Train loss: 1.073, Val loss: 1.171, Epoch time = 2.705s\n",
      "Epoch: 4, Train loss: 1.038, Val loss: 1.164, Epoch time = 2.663s\n",
      "Epoch: 5, Train loss: 0.997, Val loss: 1.139, Epoch time = 2.632s\n",
      "Epoch: 6, Train loss: 0.953, Val loss: 1.138, Epoch time = 2.711s\n",
      "Epoch: 7, Train loss: 0.906, Val loss: 1.137, Epoch time = 2.690s\n",
      "Epoch: 8, Train loss: 0.870, Val loss: 1.150, Epoch time = 2.760s\n",
      "Epoch: 9, Train loss: 0.837, Val loss: 1.151, Epoch time = 2.746s\n",
      "Epoch: 10, Train loss: 0.809, Val loss: 1.113, Epoch time = 2.750s\n",
      "Epoch: 11, Train loss: 0.759, Val loss: 1.132, Epoch time = 2.698s\n",
      "Epoch: 12, Train loss: 0.716, Val loss: 1.114, Epoch time = 2.752s\n",
      "Epoch: 13, Train loss: 0.673, Val loss: 1.140, Epoch time = 2.769s\n",
      "Epoch: 14, Train loss: 0.639, Val loss: 1.154, Epoch time = 2.690s\n",
      "Epoch: 15, Train loss: 0.611, Val loss: 1.153, Epoch time = 2.705s\n",
      "Epoch: 16, Train loss: 0.593, Val loss: 1.132, Epoch time = 2.751s\n",
      "Epoch: 17, Train loss: 0.567, Val loss: 1.154, Epoch time = 2.755s\n",
      "Epoch: 18, Train loss: 0.540, Val loss: 1.172, Epoch time = 2.836s\n",
      "Epoch: 19, Train loss: 0.516, Val loss: 1.165, Epoch time = 2.737s\n",
      "Epoch: 20, Train loss: 0.490, Val loss: 1.133, Epoch time = 2.732s\n",
      "Epoch: 21, Train loss: 0.462, Val loss: 1.189, Epoch time = 2.748s\n",
      "Epoch: 22, Train loss: 0.442, Val loss: 1.184, Epoch time = 2.769s\n",
      "Epoch: 23, Train loss: 0.424, Val loss: 1.183, Epoch time = 2.736s\n",
      "Epoch: 24, Train loss: 0.405, Val loss: 1.163, Epoch time = 2.787s\n",
      "Epoch: 25, Train loss: 0.378, Val loss: 1.200, Epoch time = 2.747s\n",
      "Epoch: 26, Train loss: 0.356, Val loss: 1.192, Epoch time = 2.827s\n",
      "Epoch: 27, Train loss: 0.340, Val loss: 1.201, Epoch time = 2.862s\n",
      "Epoch: 28, Train loss: 0.321, Val loss: 1.213, Epoch time = 2.801s\n",
      "Epoch: 29, Train loss: 0.299, Val loss: 1.221, Epoch time = 2.830s\n",
      "Epoch: 30, Train loss: 0.288, Val loss: 1.232, Epoch time = 2.938s\n",
      "Epoch: 31, Train loss: 0.269, Val loss: 1.255, Epoch time = 2.940s\n",
      "Epoch: 32, Train loss: 0.253, Val loss: 1.233, Epoch time = 2.992s\n",
      "Epoch: 33, Train loss: 0.243, Val loss: 1.241, Epoch time = 2.813s\n",
      "Epoch: 34, Train loss: 0.235, Val loss: 1.289, Epoch time = 2.864s\n",
      "Epoch: 35, Train loss: 0.222, Val loss: 1.252, Epoch time = 2.886s\n",
      "Epoch: 36, Train loss: 0.206, Val loss: 1.272, Epoch time = 4.596s\n",
      "Epoch: 37, Train loss: 0.194, Val loss: 1.285, Epoch time = 3.416s\n",
      "Epoch: 38, Train loss: 0.183, Val loss: 1.303, Epoch time = 2.985s\n",
      "Epoch: 39, Train loss: 0.172, Val loss: 1.286, Epoch time = 3.028s\n",
      "Epoch: 40, Train loss: 0.170, Val loss: 1.301, Epoch time = 3.663s\n",
      "Epoch: 41, Train loss: 0.161, Val loss: 1.243, Epoch time = 3.132s\n",
      "Epoch: 42, Train loss: 0.153, Val loss: 1.237, Epoch time = 2.818s\n",
      "Epoch: 43, Train loss: 0.146, Val loss: 1.226, Epoch time = 2.798s\n",
      "Epoch: 44, Train loss: 0.138, Val loss: 1.229, Epoch time = 2.858s\n",
      "Epoch: 45, Train loss: 0.134, Val loss: 1.225, Epoch time = 2.815s\n",
      "Epoch: 46, Train loss: 0.125, Val loss: 1.279, Epoch time = 2.727s\n",
      "Epoch: 47, Train loss: 0.118, Val loss: 1.262, Epoch time = 2.809s\n",
      "Epoch: 48, Train loss: 0.112, Val loss: 1.275, Epoch time = 2.846s\n",
      "Epoch: 49, Train loss: 0.106, Val loss: 1.330, Epoch time = 2.840s\n",
      "Epoch: 50, Train loss: 0.100, Val loss: 1.355, Epoch time = 2.859s\n",
      "Epoch: 51, Train loss: 0.094, Val loss: 1.335, Epoch time = 2.817s\n",
      "Epoch: 52, Train loss: 0.090, Val loss: 1.329, Epoch time = 2.748s\n",
      "Epoch: 53, Train loss: 0.085, Val loss: 1.332, Epoch time = 2.687s\n",
      "Epoch: 54, Train loss: 0.080, Val loss: 1.301, Epoch time = 2.807s\n",
      "Epoch: 55, Train loss: 0.080, Val loss: 1.296, Epoch time = 2.861s\n",
      "Epoch: 56, Train loss: 0.076, Val loss: 1.291, Epoch time = 2.777s\n",
      "Epoch: 57, Train loss: 0.073, Val loss: 1.365, Epoch time = 2.748s\n",
      "Epoch: 58, Train loss: 0.069, Val loss: 1.333, Epoch time = 2.747s\n",
      "Epoch: 59, Train loss: 0.066, Val loss: 1.354, Epoch time = 2.969s\n",
      "Epoch: 60, Train loss: 0.060, Val loss: 1.379, Epoch time = 2.902s\n",
      "Epoch: 61, Train loss: 0.059, Val loss: 1.367, Epoch time = 2.894s\n",
      "Epoch: 62, Train loss: 0.055, Val loss: 1.348, Epoch time = 2.953s\n",
      "Epoch: 63, Train loss: 0.052, Val loss: 1.354, Epoch time = 2.852s\n",
      "Epoch: 64, Train loss: 0.050, Val loss: 1.364, Epoch time = 2.853s\n",
      "Epoch: 65, Train loss: 0.049, Val loss: 1.366, Epoch time = 2.758s\n",
      "Epoch: 66, Train loss: 0.048, Val loss: 1.350, Epoch time = 3.019s\n",
      "Epoch: 67, Train loss: 0.047, Val loss: 1.351, Epoch time = 2.968s\n",
      "Epoch: 68, Train loss: 0.047, Val loss: 1.394, Epoch time = 2.717s\n",
      "Epoch: 69, Train loss: 0.047, Val loss: 1.414, Epoch time = 2.737s\n",
      "Epoch: 70, Train loss: 0.051, Val loss: 1.424, Epoch time = 2.946s\n",
      "Epoch: 71, Train loss: 0.052, Val loss: 1.431, Epoch time = 3.027s\n",
      "Epoch: 72, Train loss: 0.044, Val loss: 1.390, Epoch time = 2.912s\n",
      "Epoch: 73, Train loss: 0.043, Val loss: 1.427, Epoch time = 2.930s\n",
      "Epoch: 74, Train loss: 0.041, Val loss: 1.417, Epoch time = 3.076s\n",
      "Epoch: 75, Train loss: 0.037, Val loss: 1.396, Epoch time = 2.953s\n",
      "Epoch: 76, Train loss: 0.038, Val loss: 1.381, Epoch time = 2.821s\n",
      "Epoch: 77, Train loss: 0.037, Val loss: 1.380, Epoch time = 2.932s\n",
      "Epoch: 78, Train loss: 0.038, Val loss: 1.390, Epoch time = 2.989s\n",
      "Epoch: 79, Train loss: 0.037, Val loss: 1.406, Epoch time = 2.739s\n",
      "Epoch: 80, Train loss: 0.037, Val loss: 1.431, Epoch time = 2.827s\n",
      "Epoch: 81, Train loss: 0.034, Val loss: 1.449, Epoch time = 3.008s\n",
      "Epoch: 82, Train loss: 0.035, Val loss: 1.466, Epoch time = 3.037s\n",
      "Epoch: 83, Train loss: 0.034, Val loss: 1.477, Epoch time = 2.968s\n",
      "Epoch: 84, Train loss: 0.036, Val loss: 1.446, Epoch time = 3.106s\n",
      "Epoch: 85, Train loss: 0.036, Val loss: 1.392, Epoch time = 2.931s\n",
      "Epoch: 86, Train loss: 0.035, Val loss: 1.419, Epoch time = 2.799s\n",
      "Epoch: 87, Train loss: 0.033, Val loss: 1.381, Epoch time = 2.786s\n",
      "Epoch: 88, Train loss: 0.033, Val loss: 1.451, Epoch time = 3.000s\n",
      "Epoch: 89, Train loss: 0.033, Val loss: 1.476, Epoch time = 3.002s\n",
      "Epoch: 90, Train loss: 0.033, Val loss: 1.464, Epoch time = 2.948s\n",
      "Epoch: 91, Train loss: 0.031, Val loss: 1.509, Epoch time = 2.934s\n",
      "Epoch: 92, Train loss: 0.034, Val loss: 1.487, Epoch time = 2.864s\n",
      "Epoch: 93, Train loss: 0.033, Val loss: 1.456, Epoch time = 3.061s\n",
      "Epoch: 94, Train loss: 0.031, Val loss: 1.414, Epoch time = 3.229s\n",
      "Epoch: 95, Train loss: 0.031, Val loss: 1.471, Epoch time = 3.036s\n",
      "Epoch: 96, Train loss: 0.030, Val loss: 1.516, Epoch time = 2.881s\n",
      "Epoch: 97, Train loss: 0.030, Val loss: 1.525, Epoch time = 2.997s\n",
      "Epoch: 98, Train loss: 0.028, Val loss: 1.486, Epoch time = 3.068s\n",
      "Epoch: 99, Train loss: 0.027, Val loss: 1.446, Epoch time = 3.566s\n",
      "Epoch: 100, Train loss: 0.027, Val loss: 1.465, Epoch time = 3.165s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=0.1)\n",
    "NUM_EPOCHS = 100\n",
    "history = {\n",
    "        \"loss\": [], \n",
    "        \"val_los\": []\n",
    "        }\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    history['loss'].append(train_loss)\n",
    "    history['val_los'].append(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    # Early Stopping\n",
    "    # early_stopping(train_loss, val_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"We are at epoch:\", epoch)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(),\"models/newari-english.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_subword_tokens(tokens):\n",
    "    tokens = [token.replace(\"▁\", \" \") for token in tokens]\n",
    "    complete_word = \" \".join(tokens)\n",
    "    return complete_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\n"
     ]
    }
   ],
   "source": [
    "def translate_with_postprocessing(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    \n",
    "    tgt_tokens = [token for token in tgt_tokens if token not in [BOS_IDX, EOS_IDX]]\n",
    "    \n",
    "    tgt_tokens = torch.tensor(tgt_tokens)\n",
    "    \n",
    "    translated_sentence = detokenize_subword_tokens(\n",
    "        vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
    "    )\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "translated_sentence = translate_with_postprocessing(transformer, \"थो test ख​\")\n",
    "print(translated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
